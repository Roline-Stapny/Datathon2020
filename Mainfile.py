# -*- coding: utf-8 -*-
"""Copy of td_for_you_starter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10cTl3ma-VcVqnIvkLBRe4bXpUJSsOge5

# TD For-You Page

TD wants the hackathon experience to be top-notch for everyone. To assist in this, we want to provide a **For-You** page to our future applicants on our website tamudatathon.com. Can you use the TAMU Datathon data to make this happen?

**Ideas:**
- Visualization of Achievements
- Supervised Learning for Recommending Workshops
- Clustering for Team Building
- Natural Language Processing for Workshops Search Engine

**Datasets:**

We provide four datasets for this challenge. You are free to find your own as well.
1. TD 2020 Applicant Data
2. Bootcamps
3. Workshop Queries
4. Workshops


The details for these datasets are here: www.datalogz.io. 
1. Make an account, and for company, enter 'TAMU Datathon'
2. On the homepage, click 'Enter a Code' and enter SZMX5CNOZU
3. You should have access to the relevant tables on the left hand bar
4. Choose the desired table. Click "Definitions" to see column definitions
"""

import pandas as pd
apps = pd.read_csv('https://drive.google.com/uc?id=1mPK8_AasPMdqy3D9D0kxjjKcIXmhhcQo').set_index('userid') # read 'TD 2020 Applicant' data

apps.head(2)

apps.technology_experience.apply(eval).explode().value_counts()
#apps.technology_experience.apply(eval).explode()

df = apps
df['Python'] = 0
df['Excel'] = 0
df['NumPy'] = 0
df['Pandas'] = 0
df['SQL'] = 0
df['R'] = 0

df['Scikit-learn'] = 0
df['MATLAB'] = 0
df['cloud'] = 0
df['TensorFlow'] = 0
df['Keras'] = 0
df['Tableau'] = 0

df['full_stack'] = 0
df['Pytorch'] = 0
df['dev_ops'] = 0
df

tech_Ex_count = df.technology_experience.apply(eval).explode().value_counts()
tech_Ex_count,toList()

df.shape

list(tech_Ex_count.to_dict().values())

# import matplotlib.pyplot as plt
# plt.figure(figsize=(20,15))
# plt.title("Technology Experience of participants") 
# values = list(tech_Ex_count.to_dict().values())
# labels = list(tech_Ex_count.to_dict().keys())
# plt.pie(values, labels=labels, autopct=lambda p:f'{p:.2f}%, ({p*sum(values)/100 :.0f})')
# plt.show()

import matplotlib.pyplot as plt
# creating the dataset 

courses = list(tech_Ex_count.to_dict().keys())
values = list(tech_Ex_count.to_dict().values())
   
fig = plt.figure(figsize = (15, 5)) 
  
# creating the bar plot 
bar_plot = plt.bar(courses, values, color ='maroon',  
        width = 0.4) 

def autolabel(rects):
    for idx,rect in enumerate(bar_plot):
        height = rect.get_height()
        plt.text(rect.get_x() + rect.get_width()/2., 1.05*height,
                ""+str(values[idx]/12.50)+"%",
                ha='center', va='bottom', rotation=0)

autolabel(bar_plot)
  
plt.xlabel("Technology Experience") 
plt.ylabel("No. of Applicants") 
plt.title("Technology Experience of Datathon applicants (Total 1250 Applicants)") 
plt.show()

#df.loc['Excel' in df.technology_experience, 'Excel'] = 1
#a = df['Excel']['Excel' in list(df.technology_experience)] = 1
df.loc[df["technology_experience"].str.contains("(Excel)") , 'Excel'] = 1
df.loc[df["technology_experience"].str.contains("(Python)") , 'Python'] = 1
df.loc[df["technology_experience"].str.contains("(Tableau)") , 'Tableau'] = 1
df.loc[df["technology_experience"].str.contains("(Pandas)") , 'Pandas'] = 1
df.loc[df["technology_experience"].str.contains("(NumPy)") , 'NumPy'] = 1
df.loc[df["technology_experience"].str.contains("(MATLAB)") , 'MATLAB'] = 1
df.loc[df["technology_experience"].str.contains("(Pytorch)") , 'Pytorch'] = 1

df.loc[df["technology_experience"].str.contains("(Scikit-learn)") , 'Scikit-learn'] = 1
df.loc[df["technology_experience"].str.contains("(full_stack)") , 'full_stack'] = 1
df.loc[df["technology_experience"].str.contains("(TensorFlow)") , 'TensorFlow'] = 1
df.loc[df["technology_experience"].str.contains("(R)") , 'R'] = 1
df.loc[df["technology_experience"].str.contains("(SQL)") , 'SQL'] = 1

df.loc[df["technology_experience"].str.contains("(dev_ops)") , 'dev_ops'] = 1
df.loc[df["technology_experience"].str.contains("(Keras)") , 'Keras'] = 1
df.loc[df["technology_experience"].str.contains("(cloud)") , 'cloud'] = 1

df.relavent_industries.apply(eval).explode().value_counts()

df.relavent_industries.apply(eval).explode().value_counts().to_dict()

df['aerospace'] = 0
df['consulting'] = 0
df['education'] = 0
df['energy'] = 0
df['finance'] = 0
df['healthcare'] = 0

df['insurance'] = 0
df['other'] = 0
df['public_policy'] = 0
df['retail'] = 0
df['sports'] = 0
df['technology'] = 0

df['transportation'] = 0

df

df.majors.apply(eval).explode().value_counts().to_dict()

df.loc[df["relavent_industries"].str.contains("(aerospace)") , 'aerospace'] = 1
df.loc[df["relavent_industries"].str.contains("(consulting)") , 'consulting'] = 1
df.loc[df["relavent_industries"].str.contains("(education)") , 'education'] = 1
df.loc[df["relavent_industries"].str.contains("(energy)") , 'energy'] = 1
df.loc[df["relavent_industries"].str.contains("(finance)") , 'finance'] = 1
df.loc[df["relavent_industries"].str.contains("(healthcare)") , 'healthcare'] = 1
df.loc[df["relavent_industries"].str.contains("(insurance)") , 'insurance'] = 1

df.loc[df["relavent_industries"].str.contains("(other)") , 'other'] = 1
df.loc[df["relavent_industries"].str.contains("(public_policy)") , 'public_policy'] = 1
df.loc[df["relavent_industries"].str.contains("(retail)") , 'retail'] = 1
df.loc[df["relavent_industries"].str.contains("(sports)") , 'sports'] = 1
df.loc[df["relavent_industries"].str.contains("(technology)") , 'technology'] = 1

df.loc[df["relavent_industries"].str.contains("(transportation)") , 'transportation'] = 1

a = df.minors.value_counts()
a[0:20]

df['Datascience_Related_major'] = 0
df

df.loc[df["majors"].str.contains("(Computer Science & Engineering)|(Data Science & Analytics)|(Statistics)") , 'Datascience_Related_major'] = 1

df.loc[df["minors"].str.contains("(Computer Science & Engineering)|(Data Science & Analytics)|(Statistics)") , 'Datascience_Related_major'] = 1

df

apps.head()

bootcamp = pd.read_csv('https://drive.google.com/uc?id=1sovKLesEqPbkUte_ysRP9mGQ1gFCO2ME').set_index('userid') # read 'Bootcamp' data

bootcamp

joined = bootcamp.join(apps)
joined.columns

len(joined)

joined.classification.value_counts()

import numpy as np
import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(6, 3), subplot_kw=dict(aspect="equal"))


import matplotlib.pyplot as plt

# Data to plot
labels = 'Masters(56)', 'Junior(49)', 'PhD(34)', 'Sophomore(29)', 'Senior(27)', 'Fr(13)', 'Other(6)'
sizes = [56, 49, 34, 29, 27, 13, 6]

colors = ['lightskyblue', 'lightcoral', '#B5DF00','#AD1FFF', '#BF1B00','#5FB1FF','#FFC93F']


# Plot

plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)
plt.rcParams["figure.figsize"] = (8, 7)
plt.title('Classification of the workshop attendees')
plt.axis('equal')
plt.show()

joined.majors.apply(eval)

joined.majors.apply(eval).explode()

from sklearn.preprocessing import MultiLabelBinarizer

mlb = MultiLabelBinarizer()
joined = joined.join(pd.DataFrame(mlb.fit_transform(joined.majors.apply(eval)),
                          columns=mlb.classes_,
                          index=joined.index))

joined.head()

joined.head()

"""# Visualization for Sponsorships?
Awesome sponsors make an awesome TD. Sponsors want to know why they should sponsor datathon. Can you answer this for them using data viz and EDA?
"""

import matplotlib.pyplot as plt

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))
apps.relavent_industries.apply(eval).explode().value_counts().plot.pie(ax=ax1)
ax1.set_title('Applicant Interest in Industries')
apps.majors.apply(eval).explode().value_counts()[:10].plot.pie(ax=ax2)
ax2.set_title('Top 10 Majors')

print(f'Predicting {joined.workshop.nunique()} workshops')
joined.workshop.value_counts(normalize=True).round(2)

"""# Supervised Learning for Recommending Workshops?
- Netflix recommends movies, Amazon recommends products, why can't TD recommend workshops to participants?
"""

target_name = 'workshop'
feature_names = ['datascience_experience', 'age_bin', 'classification', 'num_hackathons_attended']
target = joined[target_name]
features = joined[feature_names]

from sklearn.dummy import DummyClassifier
clf = DummyClassifier(strategy='most_frequent')

from sklearn.model_selection import cross_val_score
cross_val_score(clf, features, target).mean().round(2)

#@title MAP@K
#@markdown MAP@K is a metric you should report for your recommendation engine!

import numpy as np

def apk(actual, predicted, k):
    """
    Computes the average precision at k.
    This function computes the average prescision at k between two lists of
    items.
    Parameters
    ----------
    actual : list
             A list of elements that are to be predicted (order doesn't matter)
    predicted : list
                A list of predicted elements (order does matter)
    k : int, optional
        The maximum number of predicted elements
    Returns
    -------
    score : double
            The average precision at k over the input lists
    """
    if len(predicted)>k:
        predicted = predicted[:k]

    score = 0.0
    num_hits = 0.0

    for i,p in enumerate(predicted):
        if p in actual and p not in predicted[:i]:
            num_hits += 1.0
            score += num_hits / (i+1.0)

    if not actual:
        return 0.0

    return score / min(len(actual), k)

def mapk(actual, predicted, k=1):
    """
    Computes the mean average precision at k.
    This function computes the mean average prescision at k between two lists
    of lists of items.
    Parameters
    ----------
    actual : list
             A list of lists of elements that are to be predicted 
             (order doesn't matter in the lists)
    predicted : list
                A list of lists of predicted elements
                (order matters in the lists)
    k : int, optional
        The maximum number of predicted elements
    Returns
    -------
    score : double
            The mean average precision at k over the input lists
    """
    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])

# get the pairings of (user, list of ws attended)
actual = bootcamp.groupby('userid').workshop.apply(list)
# get the corresponding users from the applications df
reindexed = apps.loc[actual.index]

def get_ordered_predictions(clf, features):
  probs = clf.predict_proba(features)
  predicted_idx = np.argsort(probs, axis=1)[:,::-1]
  return clf.classes_[predicted_idx]

clf.fit(features, target)
preds = get_ordered_predictions(clf, reindexed[feature_names])

mapk(actual, preds, k=1)

"""# Clustering for Team Building?
- Due to COVID, finding people to team with amoungst the 1200 other particants can be daunting. Could you recommend teamates considering such things as time zone? Think Facebook/Linkedin friend recommender...

# Natural Language Processing for Finding Workshops?

- Finding the right workshop to attend can be tricky and time is valuable during a datathon. Build a search engine for TD workshops that allows particpants to use natural langauge queries to find workshops, e.g. "how to import a dataset" -> Data Wrangling
- Idea: What if you trained word embeddings on towards data science [articles](https://www.kaggle.com/dorianlazar/medium-articles-dataset)?
"""

queries = pd.read_csv('https://drive.google.com/uc?id=1ff4xFh4fl0-SvpYNeYQoNvDbzdiZfn-t')
workshops = pd.read_csv('https://drive.google.com/uc?id=10MngpIZoAGgwAk_sxoORj7WPYs74nz5Y')

queries.head(2)

workshops.head(2)

workshops.tags = workshops.tags.apply(lambda xs: xs.split(', '))

# load the Stanford GloVe model
import requests
url = 'https://td2020-static.s3.amazonaws.com/glove.6B.50d.txt'
r = requests.get(url)
open('glove.6B.50d.txt', 'wb').write(r.content)

from gensim.scripts.glove2word2vec import glove2word2vec
glove_input_file = 'glove.6B.50d.txt'
word2vec_output_file = 'glove.6B.50d.txt.word2vec'
glove2word2vec(glove_input_file, word2vec_output_file)

from gensim.models import KeyedVectors
model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)

def distance(query, tag_set):
  if len(tag_set) == 0:
    return np.inf
  lquery = query.lower().split()
  prospect = ' '.join(tag_set).lower().split()
  return model.wmdistance(lquery, prospect)

def get_best_match_workshop(query):
  return np.argmin([distance(query, ws) for ws in workshops.tags.values])

print('{0:<70}{1:<40}{2:<40}'.format('Query', 'Predicted Workshop', 'Actual Workshop'))
for i in range(20):
  query = queries.iloc[i].query
  predict_idx = get_best_match_workshop(query)
  predicted = workshops.iloc[predict_idx].workshop
  actual = queries.iloc[i].workshop
  print('{0:<70}{1:<40}{2:<40}'.format(query, predicted, actual))

"""# Host On Web App?
Show us all you got by building a dashboard webapp in Python at
[streamlit.io](https://www.streamlit.io/)!
"""

X = df.copy()

del X['other_school']
del X['workshop_suggestions']
#X.isnull().sum()

X.drop(['school', 'majors', 'minors', 'age_bin', 'technology_experience', 'relavent_industries'], axis=1, inplace=True)
X

X.num_hackathons_attended.unique()

X['classification'] = df['classification']

X["first_generation"]  = X["first_generation"].astype(int)

from sklearn.preprocessing import LabelEncoder

lb_make = LabelEncoder()
X["classification"] = lb_make.fit_transform(X["classification"])

X["num_hackathons_attended"] = df["num_hackathons_attended"]

X.classification.value_counts()

cleanup_nums = {"classification":     {"Fr":0, "So":1,"Jr": 2, "Sr": 3, "Ma":4, "PhD":5, "O":6}}
X.replace(cleanup_nums, inplace=True)
X.head()

X.num_hackathons_attended.unique()

cleanup_nums = {"num_hackathons_attended":     {"0":0, "1-3":2,"4-7": 5, "10+": 11, "8-10":8}}
X.replace(cleanup_nums, inplace=True)
X.head()

X

#sklearn imports
from sklearn.decomposition import PCA #Principal Component Analysis
from sklearn.manifold import TSNE #T-Distributed Stochastic Neighbor Embedding
from sklearn.cluster import KMeans #K-Means Clustering
from sklearn.preprocessing import StandardScaler #used for 'Feature Scaling'

#Initialize our model
kmeans = KMeans(n_clusters=3)

#Fit our model
kmeans.fit(X)

#Find which cluster each data-point belongs to
clusters = kmeans.predict(X)

#Add the cluster vector to our DataFrame, X
X["Cluster"] = clusters

X.Cluster.unique()

import numpy as np
#plotX is a DataFrame containing 5000 values sampled randomly from X
plotX = pd.DataFrame(np.array(X))

#Rename plotX's columns since it was briefly converted to an np.array above
plotX.columns = X.columns

#PCA with one principal component
pca_1d = PCA(n_components=1)

#PCA with two principal components
pca_2d = PCA(n_components=2)

#PCA with three principal components
pca_3d = PCA(n_components=3)

#This DataFrame holds that single principal component mentioned above
PCs_1d = pd.DataFrame(pca_1d.fit_transform(plotX.drop(["Cluster"], axis=1)))

#This DataFrame contains the two principal components that will be used
#for the 2-D visualization mentioned above
PCs_2d = pd.DataFrame(pca_2d.fit_transform(plotX.drop(["Cluster"], axis=1)))

#And this DataFrame contains three principal components that will aid us
#in visualizing our clusters in 3-D
PCs_3d = pd.DataFrame(pca_3d.fit_transform(plotX.drop(["Cluster"], axis=1)))

PCs_1d.columns = ["PC1_1d"]

#"PC1_2d" means: 'The first principal component of the components created for 2-D visualization, by PCA.'
#And "PC2_2d" means: 'The second principal component of the components created for 2-D visualization, by PCA.'
PCs_2d.columns = ["PC1_2d", "PC2_2d"]

PCs_3d.columns = ["PC1_3d", "PC2_3d", "PC3_3d"]

plotX = pd.concat([plotX,PCs_1d,PCs_2d,PCs_3d], axis=1, join='inner')

plotX["dummy"] = 0

#Note that all of the DataFrames below are sub-DataFrames of 'plotX'.
#This is because we intend to plot the values contained within each of these DataFrames.

cluster0 = plotX[plotX["Cluster"] == 0]
cluster1 = plotX[plotX["Cluster"] == 1]
cluster2 = plotX[plotX["Cluster"] == 2]

#plotly imports
import plotly as py
import plotly.graph_objs as go
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
#This is needed so we can display plotly plots properly
init_notebook_mode(connected=True)

import plotly.offline as pyo
import plotly.graph_objs as go
from plotly.offline import iplot

import cufflinks as cf
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot 


cf.go_offline()

init_notebook_mode(connected=False)

def configure_plotly_browser_state():
  import IPython
  display(IPython.core.display.HTML('''
        <script src="/static/components/requirejs/require.js"></script>
        <script>
          requirejs.config({
            paths: {
              base: '/static/base',
              plotly: 'https://cdn.plot.ly/plotly-1.5.1.min.js?noext',
            },
          });
        </script>
        '''))
configure_plotly_browser_state()
#trace1 is for 'Cluster 0'
trace1 = go.Scatter(
                    x = cluster0["PC1_1d"],
                    y = cluster0["dummy"],
                    mode = "markers",
                    name = "Cluster 0",
                    marker = dict(color = 'rgba(255, 128, 255, 0.8)'),
                    text = None)

#trace2 is for 'Cluster 1'
trace2 = go.Scatter(
                    x = cluster1["PC1_1d"],
                    y = cluster1["dummy"],
                    mode = "markers",
                    name = "Cluster 1",
                    marker = dict(color = 'rgba(255, 128, 2, 0.8)'),
                    text = None)

#trace3 is for 'Cluster 2'
trace3 = go.Scatter(
                    x = cluster2["PC1_1d"],
                    y = cluster2["dummy"],
                    mode = "markers",
                    name = "Cluster 2",
                    marker = dict(color = 'rgba(0, 255, 200, 0.8)'),
                    text = None)

data = [trace1, trace2, trace3]

title = "Visualizing Clusters in One Dimension Using PCA"

layout = dict(title = title,
              xaxis= dict(title= 'PC1',ticklen= 5,zeroline= False),
              yaxis= dict(title= '',ticklen= 5,zeroline= False)
             )

fig = dict(data = data, layout = layout)

#iplot(fig)
pyo.iplot(fig, filename = 'basic-line')

#Instructions for building the 2-D plot
import plotly.offline as pyo
import plotly.graph_objs as go
from plotly.offline import iplot

import cufflinks as cf
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot 


cf.go_offline()

init_notebook_mode(connected=False)

def configure_plotly_browser_state():
  import IPython
  display(IPython.core.display.HTML('''
        <script src="/static/components/requirejs/require.js"></script>
        <script>
          requirejs.config({
            paths: {
              base: '/static/base',
              plotly: 'https://cdn.plot.ly/plotly-1.5.1.min.js?noext',
            },
          });
        </script>
        '''))
configure_plotly_browser_state()

#trace1 is for 'Cluster 0'
trace1 = go.Scatter(
                    x = cluster0["PC1_2d"],
                    y = cluster0["PC2_2d"],
                    mode = "markers",
                    name = "Cluster 0",
                    marker = dict(color = 'rgba(255, 128, 255, 0.8)'),
                    text = None)

#trace2 is for 'Cluster 1'
trace2 = go.Scatter(
                    x = cluster1["PC1_2d"],
                    y = cluster1["PC2_2d"],
                    mode = "markers",
                    name = "Cluster 1",
                    marker = dict(color = 'rgba(255, 128, 2, 0.8)'),
                    text = None)

#trace3 is for 'Cluster 2'
trace3 = go.Scatter(
                    x = cluster2["PC1_2d"],
                    y = cluster2["PC2_2d"],
                    mode = "markers",
                    name = "Cluster 2",
                    marker = dict(color = 'rgba(0, 255, 200, 0.8)'),
                    text = None)

data = [trace1, trace2, trace3]

title = "Visualizing Clusters in Two Dimensions Using PCA"

layout = dict(title = title,
              xaxis= dict(title= 'PC1',ticklen= 5,zeroline= False),
              yaxis= dict(title= 'PC2',ticklen= 5,zeroline= False)
             )

fig = dict(data = data, layout = layout)

#iplot(fig)
pyo.iplot(fig, filename = 'basic-line1')

#Instructions for building the 3-D plot
import plotly.offline as pyo
import plotly.graph_objs as go
from plotly.offline import iplot

import cufflinks as cf
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot 


cf.go_offline()

init_notebook_mode(connected=False)

def configure_plotly_browser_state():
  import IPython
  display(IPython.core.display.HTML('''
        <script src="/static/components/requirejs/require.js"></script>
        <script>
          requirejs.config({
            paths: {
              base: '/static/base',
              plotly: 'https://cdn.plot.ly/plotly-1.5.1.min.js?noext',
            },
          });
        </script>
        '''))
configure_plotly_browser_state()

#trace1 is for 'Cluster 0'
trace1 = go.Scatter3d(
                    x = cluster0["PC1_3d"],
                    y = cluster0["PC2_3d"],
                    z = cluster0["PC3_3d"],
                    mode = "markers",
                    name = "Cluster 0",
                    marker = dict(color = 'rgba(255, 128, 255, 0.8)'),
                    text = None)

#trace2 is for 'Cluster 1'
trace2 = go.Scatter3d(
                    x = cluster1["PC1_3d"],
                    y = cluster1["PC2_3d"],
                    z = cluster1["PC3_3d"],
                    mode = "markers",
                    name = "Cluster 1",
                    marker = dict(color = 'rgba(255, 128, 2, 0.8)'),
                    text = None)

#trace3 is for 'Cluster 2'
trace3 = go.Scatter3d(
                    x = cluster2["PC1_3d"],
                    y = cluster2["PC2_3d"],
                    z = cluster2["PC3_3d"],
                    mode = "markers",
                    name = "Cluster 2",
                    marker = dict(color = 'rgba(0, 255, 200, 0.8)'),
                    text = None)

data = [trace1, trace2, trace3]

title = "Visualizing Clusters in Three Dimensions Using PCA"

layout = dict(title = title,
              xaxis= dict(title= 'PC1',ticklen= 5,zeroline= False),
              yaxis= dict(title= 'PC2',ticklen= 5,zeroline= False)
             )

fig = dict(data = data, layout = layout)

#iplot(fig)
pyo.iplot(fig, filename = 'basic-line1')

X.to_csv('/content/drive/My Drive/Team_clusters.csv')

from google.colab import drive
drive.flush_and_unmount
drive.mount("/content/drive/", force_remount=True)

X

len(X)

X.Datascience_Related_major.value_counts()

import matplotlib.pyplot as plt

# Data to plot
labels = 'CSCE/DataScience/Stats', 'Other'
sizes = [872, 378]
colors = ['lightskyblue', 'lightcoral']
explode = (0.1, 0)  # explode 1st slice

# Plot

plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)
plt.rcParams["figure.figsize"] = (5, 4)
plt.title('Percentange of applicants related to Datascience major/minor vs others')
plt.axis('equal')
plt.show()

